{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation of packages"
      ],
      "metadata": {
        "id": "ezgiv0WbYYMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install langchain\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "vkKmsqN-YdDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Python Packages"
      ],
      "metadata": {
        "id": "3-hiK0fyarvt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TJrP3usXYl0",
        "outputId": "fff6efa5-d1e0-4279-aed2-29d99428bde1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python:  3.9.16\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import platform\n",
        "import textwrap\n",
        "import requests\n",
        "from typing import List\n",
        "\n",
        "import openai\n",
        "import chromadb\n",
        "import langchain\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ChatVectorDBChain\n",
        "from langchain.document_loaders import GutenbergLoader\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders.base import BaseLoader\n",
        "\n",
        "print('Python: ', platform.python_version())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive on Colab"
      ],
      "metadata": {
        "id": "1NjniZC9YZEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jSeq13ffYVfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9973b2-bf95-4a3a-eef7-ce48cbe479a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI API Key"
      ],
      "metadata": {
        "id": "Lw_8ouylbJU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxx'"
      ],
      "metadata": {
        "id": "OWH1yPo_bMLp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Chroma"
      ],
      "metadata": {
        "id": "Sygktu5eYEs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"/content/drive/My Drive/Colab Notebooks/chroma/romeo\""
      ],
      "metadata": {
        "id": "6oyASQR6a3Cs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Document to Embedding"
      ],
      "metadata": {
        "id": "Vo92dxtUYE-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GutenbergLoader(BaseLoader):\n",
        "    \"\"\"Loader that uses urllib to load .txt web files.\"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str):\n",
        "        \"\"\"Initialize with file path.\"\"\"\n",
        "        if not file_path.startswith(\"https://open-academy.github.io\"):\n",
        "            raise ValueError(\"file path must start with 'https://open-academy.github.io'\")\n",
        "\n",
        "        if not file_path.endswith(\".md\"):\n",
        "            raise ValueError(\"file path must end with '.md'\")\n",
        "\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def load(self) -> List[Document]:\n",
        "        \"\"\"Load file.\"\"\"\n",
        "        from urllib.request import urlopen\n",
        "\n",
        "        elements = urlopen(self.file_path)\n",
        "        text = \"\\n\\n\".join([str(el.decode(\"utf-8-sig\")) for el in elements])\n",
        "        metadata = {\"source\": self.file_path}\n",
        "        return [Document(page_content=text, metadata=metadata)]\n",
        "\n",
        "def get_gutenberg(url):\n",
        "    loader = GutenbergLoader(url)\n",
        "    data = loader.load()\n",
        "    return data"
      ],
      "metadata": {
        "id": "jiINbdENa3gO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the text data from Project Open-academy\n",
        "modelDeployment_md = 'https://open-academy.github.io/machine-learning/_sources/machine-learning-productionization/model-deployment.md'\n",
        "modelDeployment_data = get_gutenberg(modelDeployment_md)\n",
        "\n",
        "# Initializing a TokenTextSplitter object to split the text into chunks of 1000 tokens with 0 token overlap\n",
        "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "# Splitting the Romeo and Juliet text into chunks using the TokenTextSplitter object\n",
        "modelDeployment_doc = text_splitter.split_documents(modelDeployment_data)\n",
        "\n",
        "# Initializing an OpenAIEmbeddings object for word embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Generating Chroma vectors from the text chunks using the OpenAIEmbeddings object and persisting them to disk\n",
        "vectordb = Chroma.from_documents(modelDeployment_doc, embeddings, persist_directory=persist_directory)\n",
        "# This can be used to explicitly persist the data to disk. It will also be called automatically when the object is destroyed.\n",
        "vectordb.persist()"
      ],
      "metadata": {
        "id": "5JwgArJYa8AU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd36370c-c863-4c9e-a905-62362ea5e155"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /content/drive/My Drive/Colab Notebooks/chroma/romeo\n",
            "Exception ignored in: <function PersistentDuckDB.__del__ at 0x7fcf81f07ca0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/chromadb/db/duckdb.py\", line 446, in __del__\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/chromadb/db/duckdb.py\", line 399, in persist\n",
            "duckdb.IOException: IO Error: Could not rename file!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelDeployment_data"
      ],
      "metadata": {
        "id": "Cpjb5CUpybhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure LangChain QA"
      ],
      "metadata": {
        "id": "7tfc8C5ha31r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "romeoandjuliet_qa = ChatVectorDBChain.from_llm(OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"), vectordb, return_source_documents=True)"
      ],
      "metadata": {
        "id": "YqdQKtFba_Gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3768e62e-84af-4155-c0c4-f597fbf9f948"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/langchain/llms/openai.py:623: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/langchain/chains/conversational_retrieval/base.py:191: UserWarning: `ChatVectorDBChain` is deprecated - please use `from langchain.chains import ConversationalRetrievalChain`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Romeo and Juliet\n",
        "query = \"Have Romeo and Juliet spent the night together? Provide a verbose answer, referencing passages from the book.\"\n",
        "chat_history = ''\n",
        "result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})"
      ],
      "metadata": {
        "id": "T9qDaY0IBGkY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to deployment the data model? Provide a verbose answer, referencing passages from the book.\"\n",
        "chat_history = ''\n",
        "result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})"
      ],
      "metadata": {
        "id": "CVQ_xHNRLeMs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"source_documents\"] # Vector search engine result"
      ],
      "metadata": {
        "id": "gcl3UbvqB4H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"answer\"] # Answer"
      ],
      "metadata": {
        "id": "ifoLr3S3CEyq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "2b290740-b82b-4343-8e4f-74989e2d9586"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are several patterns for deploying a Machine Learning model, including \"model as module,\" \"model as service,\" and \"model as data.\" In the \"model as module\" approach, the model is embedded as a dependency in the application and packaged together as a module. In the \"model as service\" approach, the model is wrapped in a service that can be deployed independently of the application, allowing for independent updates of the model and application. In the \"model as data\" approach, the model is treated and published independently, and the application ingests it as data at runtime instead. The book also discusses the importance of version control and automated CI/CD pipelines in the deployment process, as well as the challenges of coordinating scientists, software engineers, data engineers, and business professionals. Additionally, the book covers the evolution of deployment strategies, from basic deployment to container orchestration-based deployment, and the use of Machine Learning as a service (MLaaS) for deploying Machine Learning solutions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def markdown_to_python(markdown_text):\n",
        "    # Escape quotes and backslashes in the input\n",
        "    escaped_input = markdown_text.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n",
        "\n",
        "    # Generate the Python string\n",
        "    python_string = f\"'{escaped_input}'\"\n",
        "\n",
        "    return python_string"
      ],
      "metadata": {
        "id": "s1m_iQS199dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_text = \"Generating questions and answers from the book is a straightforward process. To assess the accuracy of the results, I will be comparing the answers with those from SparkNotes. > *SparkNotes editors.* [“Romeo and Juliet” SparkNotes.com](https://www.sparknotes.com/shakespeare/romeojuliet/key-questions-and-answers/), *SparkNotes LLC, 2005* >\"\n",
        "query = markdown_to_python(markdown_text);\n",
        "result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "chat_history = chat_history + result[\"answer\"]\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "id": "5J6B5FTC80nv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4896682-4381-45aa-9286-82aef7a994ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is not a question, it is a statement.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restart the conversation\n",
        "chat_history = [(\"hello\", \"hello\")]\n",
        "count = 0"
      ],
      "metadata": {
        "id": "wRK1QcjmIfom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st\n",
        "markdown_text = \"I get a number '23333', please give me the completed code in Python which could change the number into a string. In the code.\"\n",
        "\n",
        "query = markdown_to_python(markdown_text)\n",
        "result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "chat_history = chat_history + [(query, result[\"answer\"])]\n",
        "formatted_history = \"\\n\".join([f\"Question: {q}\\nAnswer: {a}\" for q, a in chat_history])\n",
        "wrapped_history = textwrap.fill(formatted_history, width=120)\n",
        "print(wrapped_history + \"\\n\")\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "id": "D0jlxG4m_fLd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ba2ac8db-4b15-4845-ccdb-1fbc6107637a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: hello Answer: hello Question: 'I get a number \\'23333\\', please give me the complete code in Python which\n",
            "could change the number into a string.' Answer: str(23333) Question: 'I get a number \\'23333\\', please give me the\n",
            "complete code in Python which could change the number into a string. In the code, I need you use for loop.' Answer: Yes,\n",
            "here is the code:  num = 23333 string = \"\"  for digit in str(num):     string += digit  print(string)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, here is the code:\\n\\nnum = 23333\\nstring = \"\"\\n\\nfor digit in str(num):\\n    string += digit\\n\\nprint(string)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd\n",
        "query = \"However, you can not uss the str function. Show me the code again.\"\n",
        "result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "chat_history = chat_history + [(query, result[\"answer\"])]\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lFy_1S2rOfOc",
        "outputId": "55d7db0e-1d59-44cb-d1e5-610d238dd1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, here is a code in Python that can change the number \\'23333\\' into a string using a for loop, but without using the str function:\\n\\n```\\nnum = 23333\\nstring = \"\"\\n\\nfor digit in str(num):\\n    string += chr(ord(\\'0\\') + int(digit))\\n\\nprint(string)\\n```\\n\\nThis code converts each digit of the number into its corresponding ASCII character code and then concatenates them to form a string.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd\n",
        "query = \"Romeo and Juliet are not lovers? Provide a verbose answer, referencing passages from the book.\"\n",
        "result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "chat_history = chat_history + [(query, result[\"answer\"])]\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "BnnoXs-ePKp6",
        "outputId": "518bebec-6373-4822-8df9-040609db859c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No, there is no evidence in the given context that suggests Romeo and Juliet are not lovers. On the contrary, the Chorus describes them as being in love and the dialogue between Juliet and her Nurse reveals her intense feelings for Romeo despite him being a member of the enemy Montague family.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restart the conversation\n",
        "chat_history = [(\"\", \"\")]\n",
        "count = 0\n",
        "\n",
        "# while loop for typing\n",
        "while 1:\n",
        "  markdown_text = input(\"\\nQuery[{}]:\".format(count))\n",
        "  query = markdown_to_python(markdown_text)\n",
        "  result = romeoandjuliet_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "  chat_history = chat_history + [(query, result[\"answer\"])]\n",
        "  formatted_history = \"\\n\".join([f\"Question: {q}\\nAnswer: {a}\" for q, a in chat_history])\n",
        "  wrapped_history = textwrap.fill(formatted_history, width=120)\n",
        "  print(wrapped_history + \"\\n\")\n",
        "  result[\"answer\"]"
      ],
      "metadata": {
        "id": "SZ8lX373WtSE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}